{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "67974f4f-6b95-4e78-a968-b4fa5eb528ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Create a discussion file path list\n",
    "discussion_file1_path = \"../Week1/DevGPT/snapshot_20230727/20230727_195954_discussion_sharings.json\"\n",
    "discussion_file2_path = \"../Week1/DevGPT/snapshot_20230803/20230803_094811_discussion_sharings.json\"\n",
    "discussion_file3_path = \"../Week1/DevGPT/snapshot_20230810/20230810_124048_discussion_sharings.json\"\n",
    "discussion_file4_path = \"../Week1/DevGPT/snapshot_20230817/20230817_130721_discussion_sharings.json\"\n",
    "discussion_file5_path = \"../Week1/DevGPT/snapshot_20230824/20230824_102000_discussion_sharings.json\"\n",
    "discussion_file6_path = \"../Week1/DevGPT/snapshot_20230831/20230831_061926_discussion_sharings.json\"\n",
    "\n",
    "discussion_json_files = [discussion_file1_path,discussion_file2_path,discussion_file3_path,discussion_file4_path,discussion_file5_path,discussion_file6_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "24f9ff8f-fd66-4ecf-920d-51c8a1b8b708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an issue file path list\n",
    "issue_file1_path = \"../Week1/DevGPT/snapshot_20230727/20230727_195941_issue_sharings.json\"\n",
    "issue_file2_path = \"../Week1/DevGPT/snapshot_20230803/20230803_094705_issue_sharings.json\"\n",
    "issue_file3_path = \"../Week1/DevGPT/snapshot_20230810/20230810_123938_issue_sharings.json\"\n",
    "issue_file4_path = \"../Week1/DevGPT/snapshot_20230817/20230817_130502_issue_sharings.json\"\n",
    "issue_file5_path = \"../Week1/DevGPT/snapshot_20230824/20230824_101836_issue_sharings.json\"\n",
    "issue_file6_path = \"../Week1/DevGPT/snapshot_20230831/20230831_061759_issue_sharings.json\"\n",
    "\n",
    "issue_json_files = [issue_file1_path,issue_file2_path,issue_file3_path,issue_file4_path,issue_file5_path,issue_file6_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d0801995-288c-49c7-9696-6b8f88328d64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../Week1/DevGPT/snapshot_20230727/20230727_195954_discussion_sharings.json',\n",
       " '../Week1/DevGPT/snapshot_20230803/20230803_094811_discussion_sharings.json',\n",
       " '../Week1/DevGPT/snapshot_20230810/20230810_124048_discussion_sharings.json',\n",
       " '../Week1/DevGPT/snapshot_20230817/20230817_130721_discussion_sharings.json',\n",
       " '../Week1/DevGPT/snapshot_20230824/20230824_102000_discussion_sharings.json',\n",
       " '../Week1/DevGPT/snapshot_20230831/20230831_061926_discussion_sharings.json',\n",
       " '../Week1/DevGPT/snapshot_20230727/20230727_195941_issue_sharings.json',\n",
       " '../Week1/DevGPT/snapshot_20230803/20230803_094705_issue_sharings.json',\n",
       " '../Week1/DevGPT/snapshot_20230810/20230810_123938_issue_sharings.json',\n",
       " '../Week1/DevGPT/snapshot_20230817/20230817_130502_issue_sharings.json',\n",
       " '../Week1/DevGPT/snapshot_20230824/20230824_101836_issue_sharings.json',\n",
       " '../Week1/DevGPT/snapshot_20230831/20230831_061759_issue_sharings.json']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate two types of files\n",
    "json_files = discussion_json_files + issue_json_files\n",
    "json_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "609e09f5-c534-46be-aa9b-efa08f1625db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "RangeIndex: 1998 entries, 0 to 1997\n",
      "Series name: Sources\n",
      "Non-Null Count  Dtype \n",
      "--------------  ----- \n",
      "1998 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 15.7+ KB\n",
      "None\n",
      "0    {'Type': 'discussion', 'URL': 'https://github....\n",
      "1    {'Type': 'discussion', 'URL': 'https://github....\n",
      "2    {'Type': 'discussion', 'URL': 'https://github....\n",
      "3    {'Type': 'discussion', 'URL': 'https://github....\n",
      "4    {'Type': 'discussion', 'URL': 'https://github....\n",
      "Name: Sources, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Load the data into DataFrame from all files \n",
    "dataframes = []\n",
    "for file in json_files:\n",
    "    try:\n",
    "        # load json files\n",
    "        df = pd.read_json(file)\n",
    "        dataframes.append(df[\"Sources\"])\n",
    "    except ValueError as e:\n",
    "        print(f\"error from {file} : {e}\")\n",
    "\n",
    "# combine all DataFrames\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "print(combined_df.info())\n",
    "print(combined_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6dbe9d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type            0\n",
      "RepoLanguage    0\n",
      "Number          0\n",
      "UpvoteCount     0\n",
      "PromptTokens    0\n",
      "AnswerTokens    0\n",
      "Newline         0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['discussion', 'issue'], dtype=object)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize lists to collect extracted data\n",
    "Type_list = []\n",
    "RepoLanguage_list = []\n",
    "Number_list = []\n",
    "UpvoteCount_list = []\n",
    "Conversations_num_list = []\n",
    "PromptTokens_list = []\n",
    "AnswerTokens_list = []\n",
    "Newline=[]\n",
    "\n",
    "# Iterate through the rows of the DataFrame\n",
    "for i in range(len(combined_df)):\n",
    "    # Extract Type, RepoLanguage, Number, and UpvoteCount from the current row\n",
    "    row = combined_df.iloc[i]\n",
    "    type_value = row.get(\"Type\", \"unknown\")  # Extract \"Type\" field\n",
    "    repo_language = row.get(\"RepoLanguage\", \"unknown\")\n",
    "    number = row.get(\"Number\", 0)\n",
    "    upvote_count = row.get(\"UpvoteCount\", 0)\n",
    "    \n",
    "    # Extract ChatgptSharing field\n",
    "    ChatgptSharing = row.get(\"ChatgptSharing\", [])\n",
    "    total_conversations = 0  # Counter for the number of conversations in a record\n",
    "    \n",
    "    # Process each ChatgptSharing item\n",
    "    for item in ChatgptSharing:\n",
    "        Conversations = item.get(\"Conversations\", [])\n",
    "        total_conversations += len(Conversations)  # Count the number of conversation turns\n",
    "        \n",
    "        # Process each conversation (Prompt and Answer)\n",
    "        for conv in Conversations:\n",
    "            prompt = conv.get(\"Prompt\", \"\")\n",
    "            answer = conv.get(\"Answer\", \"\")\n",
    "            \n",
    "            # Count tokens using split()\n",
    "            prompt_tokens = len(prompt.split())  # Splitting on spaces\n",
    "            answer_tokens = len(answer.split())\n",
    "\n",
    "            prompt_newline_count = prompt.count(\"\\n\\n\")\n",
    "            # Append **per conversation** (ensuring matching lengths)\n",
    "            Type_list.append(type_value)  # Append type for each conversation\n",
    "            RepoLanguage_list.append(repo_language)\n",
    "            Number_list.append(number)\n",
    "            UpvoteCount_list.append(upvote_count)\n",
    "            Conversations_num_list.append(total_conversations)  # Append total conv count per conversation\n",
    "            PromptTokens_list.append(prompt_tokens)\n",
    "            AnswerTokens_list.append(answer_tokens)\n",
    "            Newline.append( prompt_newline_count)\n",
    "\n",
    "# Combine the extracted data into a DataFrame\n",
    "extracted_data = {\n",
    "    \"Type\": Type_list,\n",
    "    \"RepoLanguage\": RepoLanguage_list,\n",
    "    \"Number\": Number_list,\n",
    "    \"UpvoteCount\": UpvoteCount_list,\n",
    "    \"PromptTokens\": PromptTokens_list,\n",
    "    \"AnswerTokens\": AnswerTokens_list,\n",
    "    \"Newline\": Newline\n",
    "}\n",
    "df = pd.DataFrame(extracted_data)\n",
    "# Display the resulting DataFrame\n",
    "df.isnull().sum()\n",
    "df[\"RepoLanguage\"]=df[\"RepoLanguage\"].fillna(\"other\")\n",
    "print(df.isnull().sum())\n",
    "df[\"Type\"].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fae4f300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type            0\n",
      "RepoLanguage    0\n",
      "Number          0\n",
      "UpvoteCount     0\n",
      "PromptTokens    0\n",
      "AnswerTokens    0\n",
      "Newline         0\n",
      "KeywordCount    0\n",
      "dtype: int64\n",
      "         Type RepoLanguage  Number  UpvoteCount  PromptTokens  AnswerTokens  \\\n",
      "0  discussion        other      27            1            15           341   \n",
      "1  discussion        other      27            1            15           341   \n",
      "2  discussion        other      74            1          1690           184   \n",
      "3  discussion        other      74            1           369            58   \n",
      "4  discussion       Python       7            1           358            11   \n",
      "\n",
      "   Newline  KeywordCount  \n",
      "0        0             0  \n",
      "1        0             0  \n",
      "2       84            53  \n",
      "3       17             6  \n",
      "4       11             2  \n",
      "['discussion' 'issue']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Initialize lists to collect extracted data\n",
    "Type_list = []\n",
    "RepoLanguage_list = []\n",
    "Number_list = []\n",
    "UpvoteCount_list = []\n",
    "Conversations_num_list = []\n",
    "PromptTokens_list = []\n",
    "AnswerTokens_list = []\n",
    "Newline = []\n",
    "KeywordCount_list = []  # New list to store single-word keyword count\n",
    "\n",
    "# Define the list of single-word keywords\n",
    "keywords = [\n",
    "    # Instructional & Step-by-Step Words\n",
    "    \"list\", \"enumerate\", \"examples\", \"overview\", \"summary\", \"keypoints\", \"factors\",\n",
    "    \"explain\", \"describe\", \"steps\", \"process\", \"how\", \"why\", \"detailed\", \"tutorial\",\n",
    "    \"instruction\", \"methodology\", \"workflow\", \"structure\",\n",
    "\n",
    "    # Comparative & Analytical Terms\n",
    "    \"comparison\", \"contrast\", \"difference\", \"analysis\", \"evaluate\", \"impact\", \n",
    "    \"relationship\", \"correlation\", \"variance\", \"distribution\", \"hypothesis\",\n",
    "    \"assessment\", \"benchmark\", \"criteria\", \"metrics\", \"performance\",\n",
    "\n",
    "    # Theoretical & Conceptual Words\n",
    "    \"reasoning\", \"theory\", \"principles\", \"fundamentals\", \"concepts\", \"philosophy\",\n",
    "    \"mechanism\", \"frameworks\", \"paradigm\", \"approach\", \"perspective\",\n",
    "\n",
    "    # Machine Learning & AI Terms\n",
    "    \"gradient\", \"deep\", \"learning\", \"neural\", \"networks\", \"optimization\", \n",
    "    \"regularization\", \"probability\", \"statistics\", \"machine\", \"transformers\",\n",
    "    \"classification\", \"prediction\", \"reinforcement\", \"hyperparameters\",\n",
    "    \"activation\", \"backpropagation\", \"clustering\", \"feature\", \"parameter\",\n",
    "    \"regression\", \"outliers\", \"sampling\", \"estimation\", \"loss function\",\n",
    "    \"interpretability\", \"generalization\", \"convergence\",\n",
    "\n",
    "    # Example & Scenario-Based Words\n",
    "    \"example\", \"use case\", \"scenario\", \"application\", \"illustration\", \n",
    "    \"simulation\", \"demonstration\", \"real-world\", \"experiment\",\n",
    "\n",
    "    # Advanced Technical Terms\n",
    "    \"schedule\", \"implementation\", \"explanation\", \"hypothesis\", \"derivatives\",\n",
    "    \"decision tree\", \"random forest\", \"support vector\", \"Bayesian\", \"Markov\",\n",
    "    \"algorithm\", \"model\", \"training\", \"evaluation\", \"reinforcement learning\"\n",
    "]\n",
    "\n",
    "\n",
    "# Compile regex pattern for single-word keyword matching\n",
    "pattern = r'\\b(?:' + '|'.join(map(re.escape, keywords)) + r')\\b'\n",
    "\n",
    "# Iterate through the DataFrame rows\n",
    "for i in range(len(combined_df)):\n",
    "    # Extract row values for Type, RepoLanguage, Number, and UpvoteCount\n",
    "    row = combined_df.iloc[i]\n",
    "    type_value = row.get(\"Type\", \"unknown\")  \n",
    "    repo_language = row.get(\"RepoLanguage\", \"unknown\")\n",
    "    number = row.get(\"Number\", 0)\n",
    "    upvote_count = row.get(\"UpvoteCount\", 0)\n",
    "    \n",
    "    # Extract ChatgptSharing field\n",
    "    ChatgptSharing = row.get(\"ChatgptSharing\", [])\n",
    "    total_conversations = 0  # Counter for conversation count per record\n",
    "    \n",
    "    # Process each ChatgptSharing item\n",
    "    for item in ChatgptSharing:\n",
    "        Conversations = item.get(\"Conversations\", [])\n",
    "        total_conversations += len(Conversations)  # Count number of conversations\n",
    "        \n",
    "        # Process each conversation (Prompt and Answer)\n",
    "        for conv in Conversations:\n",
    "            prompt = conv.get(\"Prompt\", \"\")\n",
    "            answer = conv.get(\"Answer\", \"\")\n",
    "\n",
    "            # Count tokens in Prompt and Answer\n",
    "            prompt_tokens = len(prompt.split())  \n",
    "            answer_tokens = len(answer.split())\n",
    "\n",
    "            # Count occurrences of newline (\"\\n\\n\") in the prompt\n",
    "            prompt_newline_count = prompt.count(\"\\n\\n\")\n",
    "\n",
    "            # Count the number of single-word keywords in the Prompt\n",
    "            prompt_keyword_count = len(re.findall(pattern, prompt.lower())) if prompt else 0\n",
    "\n",
    "            # Append values for each conversation\n",
    "            Type_list.append(type_value)  \n",
    "            RepoLanguage_list.append(repo_language)\n",
    "            Number_list.append(number)\n",
    "            UpvoteCount_list.append(upvote_count)\n",
    "            Conversations_num_list.append(total_conversations)  \n",
    "            PromptTokens_list.append(prompt_tokens)\n",
    "            AnswerTokens_list.append(answer_tokens)\n",
    "            Newline.append(prompt_newline_count)\n",
    "            KeywordCount_list.append(prompt_keyword_count)  \n",
    "\n",
    "# Combine extracted data into a DataFrame\n",
    "extracted_data = {\n",
    "    \"Type\": Type_list,\n",
    "    \"RepoLanguage\": RepoLanguage_list,\n",
    "    \"Number\": Number_list,\n",
    "    \"UpvoteCount\": UpvoteCount_list,\n",
    "    \"PromptTokens\": PromptTokens_list,\n",
    "    \"AnswerTokens\": AnswerTokens_list,\n",
    "    \"Newline\": Newline,\n",
    "    \"KeywordCount\": KeywordCount_list  # New column for keyword count\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(extracted_data)\n",
    "\n",
    "# Fill missing values in RepoLanguage\n",
    "df[\"RepoLanguage\"] = df[\"RepoLanguage\"].fillna(\"other\")\n",
    "\n",
    "# Display summary of missing values and unique Type values\n",
    "print(df.isnull().sum())\n",
    "print(df.head())\n",
    "print(df[\"Type\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a55a7593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Type  RepoLanguage  Number  UpvoteCount  PromptTokens  AnswerTokens  \\\n",
      "0     0            34      27            1            15           341   \n",
      "1     0            34      27            1            15           341   \n",
      "2     0            34      74            1          1690           184   \n",
      "3     0            34      74            1           369            58   \n",
      "4     0            23       7            1           358            11   \n",
      "\n",
      "   Newline  KeywordCount  \n",
      "0        0             0  \n",
      "1        0             0  \n",
      "2       84            53  \n",
      "3       17             6  \n",
      "4       11             2  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "label_encoders = {}\n",
    "for col in [\"Type\", \"RepoLanguage\"]:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le \n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bc60f1de-0c3b-436e-90f7-5667e998734c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best k value found: 1\n",
      "Root Test Mean Squared Error (MSE): 34.1366\n",
      "Test R-squared (R²): 0.9157\n",
      "Mean Absolute Percentage Error (MAPE): 15.82%\n",
      "Prediction Accuracy: 84.18%\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Define predictors (X) and target variable (y)\n",
    "X = df.drop(columns=[\"AnswerTokens\"])\n",
    "y = df[\"AnswerTokens\"]  \n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=65)\n",
    "\n",
    "param_grid = {\"n_neighbors\": np.arange(1, 30, 1)}  # Try k values from 1 to 29 (odd numbers)\n",
    "\n",
    "knn = KNeighborsRegressor()\n",
    "grid_search = GridSearchCV(knn, param_grid, cv=5, scoring=\"neg_mean_squared_error\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best k value\n",
    "best_k = grid_search.best_params_[\"n_neighbors\"]\n",
    "print(f\"Best k value found: {best_k}\")\n",
    "\n",
    "knn_best = KNeighborsRegressor(n_neighbors=best_k)\n",
    "knn_best.fit(X_train, y_train)\n",
    "\n",
    "y_pred = knn_best.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "y_test_no_zeros = y_test.replace(0, 1)\n",
    "mape = (abs((y_test - y_pred) / y_test_no_zeros).mean()) * 100  \n",
    "accuracy = 100 - mape  # Accuracy derived from MAPE\n",
    "\n",
    "print(f\"Root Test Mean Squared Error (MSE): {rmse:.4f}\")\n",
    "print(f\"Test R-squared (R²): {r2:.4f}\")\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n",
    "print(f\"Prediction Accuracy: {accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
